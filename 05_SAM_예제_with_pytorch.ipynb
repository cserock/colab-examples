{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cserock/colab-examples/blob/main/05_SAM_%EC%98%88%EC%A0%9C_with_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0Wut2G5kz2Z"
      },
      "source": [
        "[![Roboflow Notebooks](https://media.roboflow.com/notebooks/template/bannertest2-2.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672932710194)](https://github.com/roboflow/notebooks)\n",
        "\n",
        "# Segment Anything Model (SAM)\n",
        "\n",
        "---\n",
        "\n",
        "[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/facebookresearch/segment-anything) [![arXiv](https://img.shields.io/badge/arXiv-2304.02643-b31b1b.svg)](https://arxiv.org/abs/2304.02643)\n",
        "\n",
        "Segment Anything Model (SAM): a new AI model from Meta AI that can \"cut out\" any object, in any image, with a single click. SAM is a promptable segmentation system with zero-shot generalization to unfamiliar objects and images, without the need for additional training. This notebook is an extension of the [official notebook](https://colab.research.google.com/github/facebookresearch/segment-anything/blob/main/notebooks/automatic_mask_generator_example.ipynb) prepared by Meta AI.\n",
        "\n",
        "![segment anything model](https://media.roboflow.com/notebooks/examples/segment-anything-model-paper.png)\n",
        "\n",
        "## Complementary Materials\n",
        "\n",
        "---\n",
        "\n",
        "[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-segment-anything-with-sam.ipynb) [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/D-D6ZmadzPE) [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/how-to-use-segment-anything-model-sam)\n",
        "\n",
        "We recommend that you follow along in this notebook while reading the blog post on Segment Anything Model.\n",
        "\n",
        "![segment anything model blogpost](https://media.roboflow.com/notebooks/examples/segment-anything-model-blogpost.png)\n",
        "\n",
        "## Pro Tip: Use GPU Acceleration\n",
        "\n",
        "If you are running this notebook in Google Colab, navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, and then click `Save`. This will ensure your notebook uses a GPU, which will significantly speed up model training times.\n",
        "\n",
        "## Steps in this Tutorial\n",
        "\n",
        "In this tutorial, we are going to cover:\n",
        "\n",
        "- **Before you start** - Make sure you have access to the GPU\n",
        "- Install Segment Anything Model (SAM)\n",
        "- Download Example Data\n",
        "- Load Model\n",
        "- Automated Mask Generation\n",
        "- Generate Segmentation with Bounding Box\n",
        "- Segment Anything in Roboflow Universe Dataset\n",
        "\n",
        "## Let's begin!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2s3TnW4nhjC"
      },
      "source": [
        "## Before you start\n",
        "\n",
        "Let's make sure that we have access to GPU. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, and then click `Save`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Frcrk09FhJeV"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iov3yhoRnxG2"
      },
      "source": [
        "**NOTE:** To make it easier for us to manage datasets, images and models we create a `HOME` constant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgS8jFPMnj5h"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(\"HOME:\", HOME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YN3DPGZSn57p"
      },
      "source": [
        "## Install Segment Anything Model (SAM) and other dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1H9YruJen0Q8"
      },
      "outputs": [],
      "source": [
        "!pip install -q 'git+https://github.com/facebookresearch/segment-anything.git'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3CtzYroC2Lb"
      },
      "outputs": [],
      "source": [
        "!pip install -q jupyter_bbox_widget roboflow dataclasses-json supervision==0.23.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VeYIWh1iDWW"
      },
      "source": [
        "### Download SAM weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aszw1OxBwowI"
      },
      "outputs": [],
      "source": [
        "!mkdir -p {HOME}/weights\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth -P {HOME}/weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxoFmhsHw_fG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "CHECKPOINT_PATH = os.path.join(HOME, \"weights\", \"sam_vit_h_4b8939.pth\")\n",
        "print(CHECKPOINT_PATH, \"; exist:\", os.path.isfile(CHECKPOINT_PATH))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIlYzcqqpZdc"
      },
      "source": [
        "## Download Example Data\n",
        "\n",
        "**NONE:** Let's download few example images. Feel free to use your images or videos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKv4fEE1pdKE"
      },
      "outputs": [],
      "source": [
        "!mkdir -p {HOME}/data\n",
        "\n",
        "!wget -q https://media.roboflow.com/notebooks/examples/dog.jpeg -P {HOME}/data\n",
        "!wget -q https://media.roboflow.com/notebooks/examples/dog-2.jpeg -P {HOME}/data\n",
        "!wget -q https://media.roboflow.com/notebooks/examples/dog-3.jpeg -P {HOME}/data\n",
        "!wget -q https://media.roboflow.com/notebooks/examples/dog-4.jpeg -P {HOME}/data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlhbd_f4xfiJ"
      },
      "source": [
        "## Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6_9PSZupghA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "MODEL_TYPE = \"vit_h\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n41g6y-Zx-9x"
      },
      "outputs": [],
      "source": [
        "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
        "\n",
        "sam = sam_model_registry[MODEL_TYPE](checkpoint=CHECKPOINT_PATH).to(device=DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pi3C4uDWo10h"
      },
      "source": [
        "## Automated Mask Generation\n",
        "\n",
        "To run automatic mask generation, provide a SAM model to the `SamAutomaticMaskGenerator` class. Set the path below to the SAM checkpoint. Running on CUDA and with the default model is recommended."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtymFaiKyQ57"
      },
      "outputs": [],
      "source": [
        "mask_generator = SamAutomaticMaskGenerator(sam)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0Pm0RYArgm9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "IMAGE_NAME = \"dog.jpeg\"\n",
        "IMAGE_PATH = os.path.join(HOME, \"data\", IMAGE_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdgL88fUuelk"
      },
      "source": [
        "### Generate masks with SAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u34UjLT8o7iC"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import supervision as sv\n",
        "\n",
        "image_bgr = cv2.imread(IMAGE_PATH)\n",
        "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "sam_result = mask_generator.generate(image_rgb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUNhAvdPjZ-Y"
      },
      "source": [
        "### Output format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxO265XOymA2"
      },
      "source": [
        "`SamAutomaticMaskGenerator` returns a `list` of masks, where each mask is a `dict` containing various information about the mask:\n",
        "\n",
        "* `segmentation` - `[np.ndarray]` - the mask with `(W, H)` shape, and `bool` type\n",
        "* `area` - `[int]` - the area of the mask in pixels\n",
        "* `bbox` - `[List[int]]` - the boundary box of the mask in `xywh` format\n",
        "* `predicted_iou` - `[float]` - the model's own prediction for the quality of the mask\n",
        "* `point_coords` - `[List[List[float]]]` - the sampled input point that generated this mask\n",
        "* `stability_score` - `[float]` - an additional measure of mask quality\n",
        "* `crop_box` - `List[int]` - the crop of the image used to generate this mask in `xywh` format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRiwoYNEzbBN"
      },
      "outputs": [],
      "source": [
        "print(sam_result[0].keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkNDZqBEj5Cr"
      },
      "source": [
        "### Results visualisation with Supervision\n",
        "\n",
        "As of version `0.5.0` Supervision has native support for SAM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdT3xT2AkS4g"
      },
      "outputs": [],
      "source": [
        "mask_annotator = sv.MaskAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
        "\n",
        "detections = sv.Detections.from_sam(sam_result=sam_result)\n",
        "\n",
        "annotated_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n",
        "\n",
        "sv.plot_images_grid(\n",
        "    images=[image_bgr, annotated_image],\n",
        "    grid_size=(1, 2),\n",
        "    titles=['source image', 'segmented image']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsdFDDQnjhkP"
      },
      "source": [
        "### Interaction with segmentation results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CS_WhY60XMNL"
      },
      "outputs": [],
      "source": [
        "masks = [\n",
        "    mask['segmentation']\n",
        "    for mask\n",
        "    in sorted(sam_result, key=lambda x: x['area'], reverse=True)\n",
        "]\n",
        "\n",
        "sv.plot_images_grid(\n",
        "    images=masks,\n",
        "    grid_size=(8, int(len(masks) / 8)),\n",
        "    size=(16, 16)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXKPiidy9nwH"
      },
      "source": [
        "## Generate Segmentation with Bounding Box\n",
        "\n",
        "The `SamPredictor` class provides an easy interface to the model for prompting the model. It allows the user to first set an image using the `set_image` method, which calculates the necessary image embeddings. Then, prompts can be provided via the `predict` method to efficiently predict masks from those prompts. The model can take as input both point and box prompts, as well as masks from the previous iteration of prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LU1SN8_WCLny"
      },
      "outputs": [],
      "source": [
        "mask_predictor = SamPredictor(sam)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcHT2Jr0u0_K"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "IMAGE_NAME = \"dog.jpeg\"\n",
        "IMAGE_PATH = os.path.join(HOME, \"data\", IMAGE_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qS27Xlnb7MAj"
      },
      "source": [
        "### Draw Box\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSwoXkDi9uVD"
      },
      "outputs": [],
      "source": [
        "# helper function that loads an image before adding it to the widget\n",
        "\n",
        "import base64\n",
        "\n",
        "def encode_image(filepath):\n",
        "    with open(filepath, 'rb') as f:\n",
        "        image_bytes = f.read()\n",
        "    encoded = str(base64.b64encode(image_bytes), 'utf-8')\n",
        "    return \"data:image/jpg;base64,\"+encoded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFGBhRQNC0-H"
      },
      "source": [
        "**NOTE:** Execute cell below and use your mouse to draw bounding box on the image üëá"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zieb7wDZCoj2"
      },
      "outputs": [],
      "source": [
        "IS_COLAB = True\n",
        "\n",
        "if IS_COLAB:\n",
        "    from google.colab import output\n",
        "    output.enable_custom_widget_manager()\n",
        "\n",
        "from jupyter_bbox_widget import BBoxWidget\n",
        "\n",
        "widget = BBoxWidget()\n",
        "widget.image = encode_image(IMAGE_PATH)\n",
        "widget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSAhAXOULj0t"
      },
      "outputs": [],
      "source": [
        "widget.bboxes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wsy-GikiuX5l"
      },
      "source": [
        "### Generate masks with SAM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rqxt0CdkFUf8"
      },
      "source": [
        "**NOTE:** `SamPredictor.predict` method takes `np.ndarray` `box` argument in `[x_min, y_min, x_max, y_max]` format. Let's reorganise your data first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYyhnP4xFO5_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# default_box is going to be used if you will not draw any box on image above\n",
        "default_box = {'x': 68, 'y': 247, 'width': 555, 'height': 678, 'label': ''}\n",
        "\n",
        "box = widget.bboxes[0] if widget.bboxes else default_box\n",
        "box = np.array([\n",
        "    box['x'],\n",
        "    box['y'],\n",
        "    box['x'] + box['width'],\n",
        "    box['y'] + box['height']\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGxKHiK2uqtE"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import supervision as sv\n",
        "\n",
        "image_bgr = cv2.imread(IMAGE_PATH)\n",
        "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "mask_predictor.set_image(image_rgb)\n",
        "\n",
        "masks, scores, logits = mask_predictor.predict(\n",
        "    box=box,\n",
        "    multimask_output=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kV_JOjHBNnV5"
      },
      "source": [
        "### Results visualisation with Supervision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2opSwP1Np7s"
      },
      "outputs": [],
      "source": [
        "box_annotator = sv.BoxAnnotator(color=sv.Color.RED, color_lookup=sv.ColorLookup.INDEX)\n",
        "mask_annotator = sv.MaskAnnotator(color=sv.Color.RED, color_lookup=sv.ColorLookup.INDEX)\n",
        "\n",
        "detections = sv.Detections(\n",
        "    xyxy=sv.mask_to_xyxy(masks=masks),\n",
        "    mask=masks\n",
        ")\n",
        "detections = detections[detections.area == np.max(detections.area)]\n",
        "\n",
        "source_image = box_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n",
        "segmented_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n",
        "\n",
        "sv.plot_images_grid(\n",
        "    images=[source_image, segmented_image],\n",
        "    grid_size=(1, 2),\n",
        "    titles=['source image', 'segmented image']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hD2RsmjSH5Kh"
      },
      "source": [
        "### Interaction with segmentation results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0Ts3OZ3MuzP"
      },
      "outputs": [],
      "source": [
        "import supervision as v\n",
        "\n",
        "sv.plot_images_grid(\n",
        "    images=masks,\n",
        "    grid_size=(1, 4),\n",
        "    size=(16, 4)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S14Wh-hhr3wP"
      },
      "source": [
        "## Segment Anything in Roboflow Universe Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ff80d5krPlkO"
      },
      "source": [
        "### Utils Supporting Dataset Processing\n",
        "\n",
        "A couple of helper functions that, unfortunately, we have to write ourselves to facilitate the processing of COCO annotations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZSU9BpHr2gc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Union, Optional\n",
        "from dataclasses_json import dataclass_json\n",
        "from supervision import Detections\n",
        "\n",
        "\n",
        "@dataclass_json\n",
        "@dataclass\n",
        "class COCOCategory:\n",
        "    id: int\n",
        "    name: str\n",
        "    supercategory: str\n",
        "\n",
        "\n",
        "@dataclass_json\n",
        "@dataclass\n",
        "class COCOImage:\n",
        "    id: int\n",
        "    width: int\n",
        "    height: int\n",
        "    file_name: str\n",
        "    license: int\n",
        "    date_captured: str\n",
        "    coco_url: Optional[str] = None\n",
        "    flickr_url: Optional[str] = None\n",
        "\n",
        "\n",
        "@dataclass_json\n",
        "@dataclass\n",
        "class COCOAnnotation:\n",
        "    id: int\n",
        "    image_id: int\n",
        "    category_id: int\n",
        "    segmentation: List[List[float]]\n",
        "    area: float\n",
        "    bbox: Tuple[float, float, float, float]\n",
        "    iscrowd: int\n",
        "\n",
        "\n",
        "@dataclass_json\n",
        "@dataclass\n",
        "class COCOLicense:\n",
        "    id: int\n",
        "    name: str\n",
        "    url: str\n",
        "\n",
        "\n",
        "@dataclass_json\n",
        "@dataclass\n",
        "class COCOJson:\n",
        "    images: List[COCOImage]\n",
        "    annotations: List[COCOAnnotation]\n",
        "    categories: List[COCOCategory]\n",
        "    licenses: List[COCOLicense]\n",
        "\n",
        "\n",
        "def load_coco_json(json_file: str) -> COCOJson:\n",
        "    import json\n",
        "\n",
        "    with open(json_file, \"r\") as f:\n",
        "        json_data = json.load(f)\n",
        "\n",
        "    return COCOJson.from_dict(json_data)\n",
        "\n",
        "\n",
        "class COCOJsonUtility:\n",
        "    @staticmethod\n",
        "    def get_annotations_by_image_id(coco_data: COCOJson, image_id: int) -> List[COCOAnnotation]:\n",
        "        return [annotation for annotation in coco_data.annotations if annotation.image_id == image_id]\n",
        "\n",
        "    @staticmethod\n",
        "    def get_annotations_by_image_path(coco_data: COCOJson, image_path: str) -> Optional[List[COCOAnnotation]]:\n",
        "        image = COCOJsonUtility.get_image_by_path(coco_data, image_path)\n",
        "        if image:\n",
        "            return COCOJsonUtility.get_annotations_by_image_id(coco_data, image.id)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    @staticmethod\n",
        "    def get_image_by_path(coco_data: COCOJson, image_path: str) -> Optional[COCOImage]:\n",
        "        for image in coco_data.images:\n",
        "            if image.file_name == image_path:\n",
        "                return image\n",
        "        return None\n",
        "\n",
        "    @staticmethod\n",
        "    def annotations2detections(annotations: List[COCOAnnotation]) -> Detections:\n",
        "        class_id, xyxy = [], []\n",
        "\n",
        "        for annotation in annotations:\n",
        "            x_min, y_min, width, height = annotation.bbox\n",
        "            class_id.append(annotation.category_id)\n",
        "            xyxy.append([\n",
        "                x_min,\n",
        "                y_min,\n",
        "                x_min + width,\n",
        "                y_min + height\n",
        "            ])\n",
        "\n",
        "        return Detections(\n",
        "            xyxy=np.array(xyxy, dtype=int),\n",
        "            class_id=np.array(class_id, dtype=int)\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-jQ5c4TQAic"
      },
      "source": [
        "### Download Dataset from Roboflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxH9V2nHQC4M"
      },
      "outputs": [],
      "source": [
        "%cd {HOME}\n",
        "\n",
        "import roboflow\n",
        "from roboflow import Roboflow\n",
        "\n",
        "roboflow.login()\n",
        "\n",
        "rf = Roboflow()\n",
        "\n",
        "project = rf.workspace(\"hashira-fhxpj\").project(\"mri-brain-tumor\")\n",
        "dataset = project.version(1).download(\"coco\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-Xi4Dx0QuRL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "DATA_SET_SUBDIRECTORY = \"test\"\n",
        "ANNOTATIONS_FILE_NAME = \"_annotations.coco.json\"\n",
        "IMAGES_DIRECTORY_PATH = os.path.join(dataset.location, DATA_SET_SUBDIRECTORY)\n",
        "ANNOTATIONS_FILE_PATH = os.path.join(dataset.location, DATA_SET_SUBDIRECTORY, ANNOTATIONS_FILE_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8b3oKP0QyiB"
      },
      "outputs": [],
      "source": [
        "coco_data = load_coco_json(json_file=ANNOTATIONS_FILE_PATH)\n",
        "\n",
        "CLASSES = [\n",
        "    category.name\n",
        "    for category\n",
        "    in coco_data.categories\n",
        "    if category.supercategory != 'none'\n",
        "]\n",
        "\n",
        "IMAGES = [\n",
        "    image.file_name\n",
        "    for image\n",
        "    in coco_data.images\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFQwR9-aQ0kx"
      },
      "outputs": [],
      "source": [
        "CLASSES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3WqT5qTQ6tI"
      },
      "source": [
        "### Single Image Bounding Box to Mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuUKYCLnRAaN"
      },
      "outputs": [],
      "source": [
        "# set random seed to allow easy reproduction of the experiment\n",
        "\n",
        "import random\n",
        "random.seed(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHw4yH8XRCo9"
      },
      "outputs": [],
      "source": [
        "EXAMPLE_IMAGE_NAME = random.choice(IMAGES)\n",
        "EXAMPLE_IMAGE_PATH = os.path.join(dataset.location, DATA_SET_SUBDIRECTORY, EXAMPLE_IMAGE_NAME)\n",
        "\n",
        "# load dataset annotations\n",
        "annotations = COCOJsonUtility.get_annotations_by_image_path(coco_data=coco_data, image_path=EXAMPLE_IMAGE_NAME)\n",
        "ground_truth = COCOJsonUtility.annotations2detections(annotations=annotations)\n",
        "\n",
        "# small hack - coco numerate classes from 1, model from 0 + we drop first redundant class from coco json\n",
        "ground_truth.class_id = ground_truth.class_id - 1\n",
        "\n",
        "# load image\n",
        "image_bgr = cv2.imread(EXAMPLE_IMAGE_PATH)\n",
        "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# initiate annotator\n",
        "box_annotator = sv.BoxAnnotator(color=sv.Color.RED, color_lookup=sv.ColorLookup.INDEX)\n",
        "mask_annotator = sv.MaskAnnotator(color=sv.Color.RED, color_lookup=sv.ColorLookup.INDEX)\n",
        "\n",
        "# annotate ground truth\n",
        "annotated_frame_ground_truth = box_annotator.annotate(scene=image_bgr.copy(), detections=ground_truth)\n",
        "\n",
        "# run SAM inference\n",
        "mask_predictor.set_image(image_rgb)\n",
        "\n",
        "masks, scores, logits = mask_predictor.predict(\n",
        "    box=ground_truth.xyxy[0],\n",
        "    multimask_output=True\n",
        ")\n",
        "\n",
        "detections = sv.Detections(\n",
        "    xyxy=sv.mask_to_xyxy(masks=masks),\n",
        "    mask=masks\n",
        ")\n",
        "detections = detections[detections.area == np.max(detections.area)]\n",
        "\n",
        "annotated_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n",
        "\n",
        "sv.plot_images_grid(\n",
        "    images=[annotated_frame_ground_truth, annotated_image],\n",
        "    grid_size=(1, 2),\n",
        "    titles=['source image', 'segmented image']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# YOLO-8Í≥º ÏÇ¨Ïö©Î≤ï ÎπÑÍµê"
      ],
      "metadata": {
        "id": "sNn7Pke5VUXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "id": "7JAdRGFvU-Ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import supervision as sv\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Ïù¥ÎØ∏ÏßÄ Î°úÎìú\n",
        "image = cv2.imread(IMAGE_PATH)\n",
        "\n",
        "# YOLOv8 Î™®Îç∏ Î∂àÎü¨Ïò§Í∏∞\n",
        "model = YOLO(\"yolov8n.pt\")\n",
        "\n",
        "# Í≤ÄÏ∂ú Ïã§Ìñâ\n",
        "results = model(image)[0]\n",
        "\n",
        "# Supervision Ìè¨Îß∑ÏúºÎ°ú Î≥ÄÌôò\n",
        "detections = sv.Detections.from_ultralytics(results)\n",
        "\n",
        "# ÏãúÍ∞ÅÌôî\n",
        "annotated = sv.BoxAnnotator().annotate(scene=image.copy(), detections=detections)\n",
        "sv.plot_image(annotated)\n"
      ],
      "metadata": {
        "id": "4C8uEAVCU6HH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}