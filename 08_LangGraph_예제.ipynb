{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cserock/colab-examples/blob/main/08_LangGraph_%EC%98%88%EC%A0%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f21bc3ef",
      "metadata": {
        "id": "f21bc3ef"
      },
      "source": [
        "## Environment Setup\n",
        "For Colab, the helper folders need to be copied over from the repo. The below cell does this automatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c75a211e",
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        },
        "id": "c75a211e"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "# Check if the environment variable exists\n",
        "if [ -n \"$COLAB_RELEASE_TAG\" ] || [ -n \"$COLAB_GPU\" ]; then\n",
        "    echo \"Running on Google Colab. Cloning repository into temp folder...\"\n",
        "    git clone https://github.com/TuebingenAICenter/agent-tutorial.git /tmp/tmp_repo\n",
        "    echo \"Moving all helpers to project root...\"\n",
        "    mv /tmp/tmp_repo/chat_with_X_utils .\n",
        "    mv /tmp/tmp_repo/images .\n",
        "    mv /tmp/tmp_repo/env.example ./.env\n",
        "    mv /tmp/tmp_repo/requirements.txt .\n",
        "else\n",
        "    echo \"Not running on Google Colab. Skipping git clone.\"\n",
        "fi\n",
        "\n",
        "# The installation block runs regardless of environment.\n",
        "echo \"Checking for requirements.txt and installing required packages...\"\n",
        "\n",
        "# Check if requirements.txt exists in the current directory\n",
        "if [ -f \"requirements.txt\" ]; then\n",
        "    # Attempt to install with 'uv', and if it fails (exit code != 0), use 'pip' as a fallback.\n",
        "    if command -v uv &> /dev/null; then\n",
        "        echo \"uv detected. Installing with uv...\"\n",
        "        uv pip install -r requirements.txt\n",
        "    else\n",
        "        echo \"Installing with pip...\"\n",
        "        pip install -r requirements.txt\n",
        "    fi\n",
        "else\n",
        "    echo \"ERROR! requirements.txt not found! Please check for errors...\"\n",
        "fi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 환경변수 파일(.env) 설정하기\n",
        "/content/drive/MyDrive/lg-dx/에 아래 내용으로 .env 파일을 작성한 후 업로드합니다.  \n",
        "\n",
        "OPENAI_API_KEY=sk-xxxx  \n",
        "LANGCHAIN_TRACING_V2=\"true\"  \n",
        "LANGCHAIN_ENDPOINT=\"https://api.smith.langchain.com\"  \n",
        "LANGCHAIN_API_KEY=\"lsv2_xxxx  \n",
        "LANGCHAIN_PROJECT=\"lg-dx\"\n"
      ],
      "metadata": {
        "id": "i0hwVxoxoR1N"
      },
      "id": "i0hwVxoxoR1N"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google Drive 마운트"
      ],
      "metadata": {
        "id": "acqZxR0MoURa"
      },
      "id": "acqZxR0MoURa"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "CCa3mykcm4WT"
      },
      "id": "CCa3mykcm4WT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "python-dotenv 라이브러리 설치"
      ],
      "metadata": {
        "id": "eQ9rkI-boW4O"
      },
      "id": "eQ9rkI-boW4O"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv"
      ],
      "metadata": {
        "id": "LE8bF2n9m-QK"
      },
      "id": "LE8bF2n9m-QK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "환경변수 파일 로드 및 확인"
      ],
      "metadata": {
        "id": "EagcNthMoZzQ"
      },
      "id": "EagcNthMoZzQ"
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv('/content/drive/MyDrive/lg-dx/.env', override=True)\n",
        "\n",
        "import os\n",
        "openai_api_key = os.environ.get('OPENAI_API_KEY')\n",
        "print(\"openai_api_key : \" + openai_api_key)\n",
        "langchain_api_key = os.environ.get('LANGCHAIN_API_KEY')\n",
        "print(\"langchain_api_key : \" + langchain_api_key)"
      ],
      "metadata": {
        "id": "po_UBHehnDhE"
      },
      "id": "po_UBHehnDhE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d55d542a",
      "metadata": {
        "id": "d55d542a"
      },
      "source": [
        "### Setting API key\n",
        "The following cell sets the API key for accessing LLMs. The prompt will ask for `OPENROUTER_API_KEY` if it has not been set in the .env file.\n",
        "\n",
        "Optionally an OpenAI key can be set in the `.env` file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c946f73c",
      "metadata": {
        "id": "c946f73c"
      },
      "outputs": [],
      "source": [
        "import dotenv\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# Load environment variables from a .env file if it exists\n",
        "dotenv.load_dotenv()\n",
        "\n",
        "# Prompt for the API key if it's not already set\n",
        "if not os.getenv(\"OPENROUTER_API_KEY\"):\n",
        "    os.environ[\"OPENROUTER_API_KEY\"] = getpass(\n",
        "        \"Enter your OPENROUTER API key: \"\n",
        "    )\n",
        "\n",
        "if not os.environ[\"OPENROUTER_API_KEY\"]:\n",
        "    print(\"WARNING: API key not set. Please run this cell again!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(os.environ[\"OPENROUTER_API_KEY\"])"
      ],
      "metadata": {
        "id": "Hj0oJtDSl2o3"
      },
      "id": "Hj0oJtDSl2o3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "81a0b0e8",
      "metadata": {
        "id": "81a0b0e8"
      },
      "source": [
        "# Example 01: Basic LangGraph Chatbot & Tool\n",
        "\n",
        "**What:** A simple code example for a basic chatbot that has access to a multiply function.\n",
        "\n",
        "**Why:** To show you (i) basic LangGraph flow, (ii) how all the concepts we discussed stitch together\n",
        "\n",
        "**Live:** Follow along with notebook 01 from our repository. Ideally, we can all run this easily $\\implies$ setup for advanced examples later."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f63fc15",
      "metadata": {
        "id": "3f63fc15"
      },
      "source": [
        "## What we plan to create:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e81ece0",
      "metadata": {
        "id": "3e81ece0"
      },
      "source": [
        "<img src=\"https://github.com/TuebingenAICenter/agent-tutorial/blob/main/images/nb-1-overv.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1852b39f",
      "metadata": {
        "id": "1852b39f"
      },
      "source": [
        "## What it looks like in LangGraph"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee76f9fe",
      "metadata": {
        "id": "ee76f9fe"
      },
      "source": [
        "<img src=\"https://github.com/TuebingenAICenter/agent-tutorial/blob/main/images/cond-nb-1.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2782734a",
      "metadata": {
        "id": "2782734a"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "393655ee",
      "metadata": {
        "id": "393655ee"
      },
      "source": [
        "### Importing necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c415897a",
      "metadata": {
        "id": "c415897a"
      },
      "outputs": [],
      "source": [
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "from langchain_core.tools import tool"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4e19c2c",
      "metadata": {
        "id": "b4e19c2c"
      },
      "source": [
        "<!-- ### Setting API key and initializing LLM -->\n",
        "### Initializing LLM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(os.getenv(\"OPENAI_API_KEY\"))"
      ],
      "metadata": {
        "id": "dpXiFec0n0yf"
      },
      "id": "dpXiFec0n0yf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1eb9648e",
      "metadata": {
        "id": "1eb9648e"
      },
      "outputs": [],
      "source": [
        "if os.getenv(\"OPENAI_API_KEY\"):\n",
        "    llm = ChatOpenAI(\n",
        "        model=\"gpt-4.1-mini\",\n",
        "        temperature=0.7,\n",
        "        openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "    )\n",
        "else:\n",
        "    llm = ChatOpenAI(\n",
        "        model=\"gpt-4.1-mini\",\n",
        "        temperature=0.7,\n",
        "        base_url=\"https://openrouter.ai/api/v1\",\n",
        "        api_key=os.environ[\"OPENROUTER_API_KEY\"],\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed17bfd5",
      "metadata": {
        "id": "ed17bfd5"
      },
      "source": [
        "### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a155516",
      "metadata": {
        "id": "7a155516"
      },
      "outputs": [],
      "source": [
        "from chat_with_X_utils.print_utils import (\n",
        "    print_messages_from_stream_event as _print_messages_from_stream_event,\n",
        "    print_messages_from_state as _print_messages_from_state,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67eeb9b0",
      "metadata": {
        "id": "67eeb9b0"
      },
      "source": [
        "## Basic chatbot"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cf6b465",
      "metadata": {
        "id": "5cf6b465"
      },
      "source": [
        "\n",
        "#### [Reducers](https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers)\n",
        "Reducers define how keys of the state should be updated (instead of overridden)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1aa6382e",
      "metadata": {
        "id": "1aa6382e"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph.message import add_messages"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b390bd6",
      "metadata": {
        "id": "1b390bd6"
      },
      "source": [
        "### [State](https://langchain-ai.github.io/langgraph/concepts/low_level/#state)\n",
        "\n",
        "A typed dictionary that all nodes (and conditional edges) operate on. It stores information that persists between nodes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a1ebd9f",
      "metadata": {
        "id": "8a1ebd9f"
      },
      "outputs": [],
      "source": [
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b431977",
      "metadata": {
        "id": "8b431977"
      },
      "source": [
        "### [StateGraph](https://langchain-ai.github.io/langgraph/concepts/low_level/#stategraph)\n",
        "\n",
        "The \"design\" graph that operates on the `State` (not compiled yet!).\n",
        "\n",
        "Uses a `State` object to define the structure of our agentic system (which is a state machine)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9173158b",
      "metadata": {
        "id": "9173158b"
      },
      "outputs": [],
      "source": [
        "design_graph = StateGraph(State)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fd97db2",
      "metadata": {
        "id": "4fd97db2"
      },
      "source": [
        "### Tools\n",
        "\n",
        "Tools are functions designed to be called by an LLM.\n",
        "\n",
        "- We can easily build our own using the [`@tool()` decorator](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html). which wraps a function to make it easily callable.\n",
        "\n",
        "- What's nice is that we can provide the docs_string as a \"readable\" to the LLM along with the function name using `@tool(parse_docstring=True)`.\n",
        "\n",
        "Note that the LLM needs to support tool calling for this functionality.\n",
        "\n",
        "#### Tool API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ee52ba5",
      "metadata": {
        "id": "0ee52ba5"
      },
      "outputs": [],
      "source": [
        "@tool(parse_docstring=True) #parse_docstring=True allows to add descriptions for the arguments\n",
        "def multiply_two_integers(a: int, b: int) -> int:\n",
        "    \"\"\"\n",
        "    Multiply two integers.\n",
        "\n",
        "    Args:\n",
        "        a: The first integer to multiply.\n",
        "        b: The second integer to multiply.\n",
        "    \"\"\"\n",
        "    return a*b\n",
        "\n",
        "multiply_two_integers.args_schema.model_json_schema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89afa5ef",
      "metadata": {
        "id": "89afa5ef"
      },
      "outputs": [],
      "source": [
        "tools = [multiply_two_integers]\n",
        "llm_with_tools = llm.bind_tools(tools)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b84eb266",
      "metadata": {
        "id": "b84eb266"
      },
      "source": [
        "### [Nodes](https://langchain-ai.github.io/langgraph/concepts/low_level/#nodes)\n",
        "\n",
        "Functions that operate on `State` and ouptut updates to it. Usually LLM calls, that may use tools.\n",
        "\n",
        "Here we [invoke](https://python.langchain.com/docs/how_to/lcel_cheatsheet/#invoke-a-runnable) the LLM, which is a [runnable](https://python.langchain.com/docs/concepts/runnables/). A runnable is the foundational high-level LangChain abstraction that represents any language model, output parser, retriever, or compiled LangGraph graphs, amongst others. Invoking it means accepting an input and returning an output (\"run button\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10e7f0fb",
      "metadata": {
        "id": "10e7f0fb"
      },
      "outputs": [],
      "source": [
        "def chatbot(state: State):\n",
        "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
        "\n",
        "design_graph.add_node(\"chatbot\", chatbot)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed7034fa",
      "metadata": {
        "id": "ed7034fa"
      },
      "source": [
        "#### [ToolNode](https://langchain-ai.github.io/langgraph/concepts/low_level/#edges)\n",
        "\n",
        "A pre-built tool box which becomes a node in our graph. Runs the tools called in the last [AIMessage](https://python.langchain.com/docs/concepts/messages/#aimessage) and appends the resulting [ToolMessage(s)](https://python.langchain.com/docs/concepts/messages/#toolmessage) to the \"messages\" state key (or a custom key passed into ToolNode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "522fff86",
      "metadata": {
        "id": "522fff86"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolNode\n",
        "\n",
        "tool_node = ToolNode(tools=[multiply_two_integers])\n",
        "design_graph.add_node(\"multiply_tool\", tool_node)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "642b1191",
      "metadata": {
        "id": "642b1191"
      },
      "source": [
        "### [Edges](https://langchain-ai.github.io/langgraph/concepts/low_level/#end-node)\n",
        "\n",
        "Specifies how an agentic system should transition between nodes.\n",
        "\n",
        "#### [Conditional edges](https://langchain-ai.github.io/langgraph/concepts/low_level/#conditional-edges)\n",
        "\n",
        "Defined by a function operating on the state which outputs the next node to transition to.\n",
        "Here, [`tools_condition`](https://langchain-ai.github.io/langgraph/reference/agents/?h=toolnode#langgraph.prebuilt.tool_node.tools_condition) outputs either \"tools\" if a tool call is detected in the last [AIMessage](https://python.langchain.com/docs/concepts/messages/#aimessage) or `END` if it isn't."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e3e8ad7",
      "metadata": {
        "id": "4e3e8ad7"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import tools_condition\n",
        "\n",
        "# tools_condition checks the state for a tool call,\n",
        "# if a tool call exists, tools_condition == \"tools\"\n",
        "# else, tools_condition == \"__end__\"\n",
        "\n",
        "def multiply_if_llm_wants_to(state: State):\n",
        "    nxt = tools_condition(state)\n",
        "    if nxt == END:\n",
        "        return END\n",
        "    return \"multiply_tool\"\n",
        "\n",
        "\n",
        "# graph_builder.add_conditional_edges(\"chatbot\", tools_condition)\n",
        "design_graph.add_conditional_edges(\"chatbot\", multiply_if_llm_wants_to, [\"multiply_tool\", END])\n",
        "\n",
        "design_graph.add_edge(\"multiply_tool\", \"chatbot\")\n",
        "design_graph.add_edge(START, \"chatbot\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba1f151b",
      "metadata": {
        "id": "ba1f151b"
      },
      "source": [
        "### [Checkpointer](https://langchain-ai.github.io/langgraph/concepts/memory/#short-term-memory)\n",
        "\n",
        "This is an in-memory checkpointer (just uses RAM) so that the LLM does not forget what we said in the previous messages. We can also use an SQL DB for this or whatever we like.\n",
        "\n",
        "Note that we define a `config` object here, which contains a `thread_id`. This governs the thread for which we checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2a1355c",
      "metadata": {
        "id": "c2a1355c"
      },
      "outputs": [],
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "memory = MemorySaver()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2b03efb",
      "metadata": {
        "id": "d2b03efb"
      },
      "source": [
        "### Compile and Test\n",
        "\n",
        "Now `graph` is an instance of `CompiledStateGraph`, and we can run things."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "402e92a5",
      "metadata": {
        "id": "402e92a5"
      },
      "outputs": [],
      "source": [
        "compiled_graph = design_graph.compile(checkpointer=memory)\n",
        "\n",
        "try:\n",
        "    from IPython.display import Image, display\n",
        "    # This is remote call and may fail due to rate limits\n",
        "    display(Image(compiled_graph.get_graph().draw_mermaid_png()))\n",
        "except Exception:\n",
        "    print(\"Mermaid rendering failed, trying ascii art\")\n",
        "    print(compiled_graph.get_graph().print_ascii())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87cafac0",
      "metadata": {
        "id": "87cafac0"
      },
      "outputs": [],
      "source": [
        "def stream_graph_updates(user_input: str, _printed: set):\n",
        "    for event in compiled_graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}, config, stream_mode=\"values\"):\n",
        "        _print_messages_from_stream_event(event, _printed)\n",
        "\n",
        "\n",
        "_printed = set()\n",
        "while True:\n",
        "    user_input = input(\"User: \")\n",
        "    if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
        "        print(\"Goodbye!\")s\n",
        "        break\n",
        "\n",
        "    stream_graph_updates(user_input, _printed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e31b7720",
      "metadata": {
        "id": "e31b7720"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "agent-tutorial-dev",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}